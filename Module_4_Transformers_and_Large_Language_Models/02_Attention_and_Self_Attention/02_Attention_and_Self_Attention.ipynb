{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention and Self-Attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Attention is all you need_ was the influential paper published by Google Research in 2017 that led to a whole new class of NLP models named \"Transformers\" and their derivatives.\n",
    "We will go through the stelar concept that is \"Self-attention\" that was used in this paper for the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the attention mechanism introduced in the paper \"Attention is all you need\"?\n",
    "\n",
    "Before delving into the different kinds of attention discussed on the paper, we need to review some concepts on embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embeddings\n",
    "\n",
    "As discussed on the module on NLP, an _embedding_ is a mapping from a set of words or \"tokens\" to a high dimensional Euclidean space.\n",
    "In the world of Machine Learning a good embedding is one that **can encode semantical relationships between words** and whose axes **represent some quality or attribute of words** (e.g. sentiment, is it a big object, etc.). \n",
    "\n",
    "![Example of what an embedding might look like in three dimensions](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*SYiW1MUZul1NvL1kc1RxwQ.png)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Tokens and words are not exactly the same thing, other than words, a token might be a class of punctuation symbols, a suffix or prefix of words or placeholders for words not in the training dataset. \n",
    "</div>\n",
    "\n",
    "Some of the most used tools for embeddings are [word2vec](https://code.google.com/archive/p/word2vec/) and [GloVe](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Queries, Keys and Values:\n",
    "\n",
    "The focal point of the Architecture are the multi-head attention blocks.\n",
    "\n",
    "![Transformer architecture](https://daleonai.com/images/screen-shot-2021-05-06-at-12.12.21-pm.png)\n",
    "\n",
    "To understand how they work we need to understand what do we mean by \"attention\". Let's take a look at the following formula:\n",
    "$$\n",
    "\\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}(QK^T / \\sqrt{d_k})V\n",
    "$$\n",
    "This is known as _scaled dot product attention_. Let's break it down:\n",
    "\n",
    "\n",
    "- **Queries $Q$:** This is a vector of embeddings that represent what the model is looking for.\n",
    "- **Keys $K$:** Represent what is being attended to.\n",
    "- **Values $V$:** Represent the information associated with each key.\n",
    "- **dimension $d_k$**: This is the dimension of the keys vector. It is used for numerical stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** In a sentence translation task, the query might be a word in the target language, the keys might be words in the source language, and the values might be their corresponding translations. We'll implement it in simple Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Let's create a simple 2D embedding for a word-bag\n",
    "\n",
    "wordbag=['El', 'perro','tiene', 'el', 'pelo', 'suelto', 'The', 'dog', 'has', 'loose', 'hair']\n",
    "\n",
    "# For this example, we can ignore capitalization\n",
    "\n",
    "wordbag=[word.upper() for word in wordbag]\n",
    "\n",
    "def circle_embedding(dataset):\n",
    "    \"\"\"\n",
    "    Maps every element of the dataset to an element on the unit circle: list -> dict\n",
    "    \"\"\"\n",
    "    d_s=len(dataset)\n",
    "    embedding = dict(\n",
    "                 map(\n",
    "                     lambda item:\n",
    "                     (item[1], (math.cos(2*math.pi*item[0]/d_s),\n",
    "                                math.sin(2*math.pi*item[0]/d_s))),\n",
    "                     enumerate(dataset)))\n",
    "    return embedding\n",
    "\n",
    "embedding = circle_embedding(wordbag)\n",
    "\n",
    "# Let's create our Q,K and V. For this example we will translate 'Perro'. \n",
    "\n",
    "Q = embedding['PERRO']\n",
    "K = [embedding[i] for i in ['PERRO', 'TIENE', 'PELO']]\n",
    "V = [embedding[i] for i in ['DOG', 'HAS', 'HAIR']]\n",
    "\n",
    "def softmax(v):\n",
    "    \"\"\"\n",
    "    Simple softmax implementation for 1D vectors.\n",
    "    \"\"\"\n",
    "    exp_sum = sum([math.exp(e) for e in v])\n",
    "    for element in v:\n",
    "        yield math.exp(element)/exp_sum\n",
    "\n",
    "def dot_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Calculate dot attention for Q,K and V\n",
    "    \"\"\"\n",
    "    d_k = len(K)\n",
    "    v = []\n",
    "\n",
    "    for k in K:\n",
    "        v.append((Q[0]*k[0] + Q[1]*k[1])/math.sqrt(d_k))\n",
    "    s = softmax(v)\n",
    "    return list(s)\n",
    "    #return map(lambda x: (x[0]*x[1][0], x[0]*x[1][1]), zip(s,V)) -> we return the softmax score for easier visualization of the probabilities.\n",
    "\n",
    "print(dot_attention(Q,K,V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, attention by itself doesn't really have any learnable parameters, but it can be used for data augmentation in the following manner:\n",
    "\n",
    "\n",
    "- **Weighting mechanism:** Assigns weights to different parts of the input sequence.\n",
    "- **Focus on relevant information:** Allows the model to focus on the most relevant parts of the input.\n",
    "- **Example:** In a machine translation task, the attention mechanism might focus on the part of the source sentence that is most relevant to the current word in the target sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What's self-attention?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For NLP tasks, especially with predictive text tasks we need to gather **meaning** and **context** from the input.\n",
    "This is where self-attention comes into play. We augment the attention mechanism in the following manner:\n",
    "\n",
    "- The _self_ denomination for self-attention comes from the characteristic that $Q = K = V$. \n",
    "- For each of the $Q$, $K$, and $V$ vectors, we are going to have a different **embedding** this is going to be retrieved by\n",
    "the trainable square weight matrices $W^Q$, $W^K$ and $W^V$.\n",
    "\n",
    "$$\n",
    "\\operatorname{SelfAttention}(Q, K, V) = \\operatorname{Attention}(QW^Q, KW^K, VW^V).\n",
    "$$\n",
    "\n",
    "Remember that if $q \\in \\mathbb{R}^{d_q}$ is a column entry of the matrix $Q$, then $W^Q$ is a $d_q$ by $d_q$ matrix. The same goes for $K$ and $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi-Head Attention](https://www.researchgate.net/publication/359127201/figure/fig4/AS:1140931101241348@1649030580668/The-structure-of-multi-head-attention-mechanism.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual Transformer architecture uses a mechanism called multiheaded attention. This mechanism has some advantages:\n",
    "**Projections as heads:**\n",
    "\n",
    "- **Parallel attention mechanisms:** Divide the input into multiple parallel attention mechanisms.\n",
    "- **Different perspectives:** Each head can focus on different aspects of the input.\n",
    "\n",
    "**Different heads encode different meanings:**\n",
    "\n",
    "- **Diverse understanding:** Can capture different relationships between words.\n",
    "- **Improved performance:** Enhances the model's ability to understand complex language patterns.\n",
    "\n",
    "**Embeddings as a way to derive meaning of word from context:**\n",
    "\n",
    "- **Contextual understanding:** Learn to capture the meaning of words based on their context.\n",
    "- **Semantic relationships:** Represent semantic relationships between words.\n",
    "\n",
    "The multi-head attention mechanism is implemented by taking $h$ projections from the embeddings of $Q,K$ and $V$ to lower dimensional $d_k = d_{\\operatorname{model}}/h$ spaces. For each of the _heads_ \n",
    "$h_i$ we take a different set of embeddings given by $W_i^Q, W_i^K, W_i^V$, we concatenate the _self-attention_ values as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{MultiHead}(Q, K, V) &= \\operatorname{Concat}(\\mathrm{head}_1, \\ldots, \\mathrm{head}_h) \\\\\n",
    "\\text{where  } \\mathrm{head}_i &= \\operatorname{Attention}(QW_i^Q, KW_i^K, VW_i^V).\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
