{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Example: Document Summarization Using Transformers\n",
    "\n",
    "#### Objective\n",
    "The goal of this example is to summarize a document using a pre-trained summarization model from the Hugging Face `transformers` library. We will use the `facebook/bart-large-cnn` model, which is well-suited for summarization tasks.\n",
    "\n",
    "#### Steps and Explanation\n",
    "\n",
    "1. **Install Required Libraries**\n",
    "   First, ensure you have the necessary libraries installed. You need `transformers` and `torch`. Install them using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **`BartTokenizer`**: Tokenizer for the BART model.\n",
    "   - **`BartForConditionalGeneration`**: BART model for generating summaries.\n",
    "\n",
    "3. **Load the Pre-Trained Model and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **`BartTokenizer`**: Tokenizer to convert text into tokens suitable for the BART model.\n",
    "   - **`BartForConditionalGeneration`**: Pre-trained BART model for conditional generation tasks like summarization.\n",
    "\n",
    "4. **Prepare the Document**\n",
    "   Define the document that you want to summarize. For this example, we’ll use a long text passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document = \"\"\"\n",
    "Artificial Intelligence (AI) is a broad field of computer science focused on creating systems capable of performing tasks that typically require human intelligence. These tasks include understanding natural language, recognizing patterns, solving complex problems, and making decisions. AI is divided into various subfields, including machine learning, neural networks, robotics, and natural language processing. Machine learning, a core component of AI, involves training algorithms to recognize patterns and make predictions based on data. Neural networks, inspired by the human brain, are used to model complex relationships in data. Robotics involves creating machines that can perform tasks autonomously or semi-autonomously. Natural language processing focuses on enabling machines to understand and interact using human language. AI technology has rapidly evolved over the past few decades, leading to advancements in areas such as self-driving cars, virtual assistants, and medical diagnostics. As AI continues to develop, it holds the potential to transform various industries and improve the quality of life for people around the world.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   - **`document`**: The text that we want to summarize.\n",
    "\n",
    "5. **Tokenize the Document**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(document, return_tensors=\"pt\", max_length=1024, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **`tokenizer`**: Converts the document into token IDs. We use `max_length` to ensure that the document fits within the model's input size and `truncation` to handle longer texts.\n",
    "\n",
    "6. **Generate the Summary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(\n",
    "      inputs[\"input_ids\"],\n",
    "      attention_mask=inputs[\"attention_mask\"],\n",
    "      max_length=150,\n",
    "      min_length=40,\n",
    "      length_penalty=2.0,\n",
    "      num_beams=4,\n",
    "      early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **`model.generate`**: Generates the summary from the input tokens. Key parameters include:\n",
    "     - **`max_length`**: Maximum length of the generated summary.\n",
    "     - **`min_length`**: Minimum length of the generated summary.\n",
    "     - **`length_penalty`**: Adjusts the length of the summary (higher values produce shorter summaries).\n",
    "     - **`num_beams`**: Number of beams for beam search (controls the quality of the generated summary).\n",
    "     - **`early_stopping`**: Stops generation when at least `min_length` is reached.\n",
    "\n",
    "7. **Decode and Print the Summary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **`tokenizer.decode`**: Converts the token IDs back into a human-readable string. The `skip_special_tokens` parameter ensures that special tokens used by the model are not included in the final output.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "1. **Pre-Trained Models**: We use `facebook/bart-large-cnn`, a BART model fine-tuned for summarization. BART (Bidirectional and Auto-Regressive Transformers) is effective for generating summaries because it combines bidirectional context (like BERT) with auto-regressive generation (like GPT).\n",
    "\n",
    "2. **Tokenization**: Tokenizing the document ensures that it is in a format that the model can process. We handle long documents by truncating them to fit within the model’s maximum input length.\n",
    "\n",
    "3. **Generation Parameters**: Parameters like `max_length`, `min_length`, and `length_penalty` control the length and quality of the generated summary. Adjust these parameters based on the document and desired summary length.\n",
    "\n",
    "4. **Handling Long Documents**: For very long documents, you may need to split the text into smaller chunks and summarize each chunk separately, as the model has a maximum input length it can handle.\n",
    "\n",
    "#### Complete Code Example\n",
    "\n",
    "Here is the complete code snippet for summarizing a document using BART:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Define the document to summarize\n",
    "document = \"\"\"\n",
    "Artificial Intelligence (AI) is a broad field of computer science focused on creating systems capable of performing tasks that typically require human intelligence. These tasks include understanding natural language, recognizing patterns, solving complex problems, and making decisions. AI is divided into various subfields, including machine learning, neural networks, robotics, and natural language processing. Machine learning, a core component of AI, involves training algorithms to recognize patterns and make predictions based on data. Neural networks, inspired by the human brain, are used to model complex relationships in data. Robotics involves creating machines that can perform tasks autonomously or semi-autonomously. Natural language processing focuses on enabling machines to understand and interact using human language. AI technology has rapidly evolved over the past few decades, leading to advancements in areas such as self-driving cars, virtual assistants, and medical diagnostics. As AI continues to develop, it holds the potential to transform various industries and improve the quality of life for people around the world.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the document\n",
    "inputs = tokenizer(document, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=150,\n",
    "    min_length=40,\n",
    "    length_penalty=2.0,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode and print the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to use a pre-trained summarization model to generate a concise summary of a longer document. By following these steps, you can effectively summarize documents for various applications, such as content summarization, report generation, or information extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
