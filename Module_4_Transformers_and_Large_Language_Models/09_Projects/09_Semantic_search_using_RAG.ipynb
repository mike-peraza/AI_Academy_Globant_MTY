{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Example: Semantic Search with Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "This example demonstrates how to build a simple semantic search system using a Retrieval-Augmented Generation (RAG) model from Hugging Face's Transformers library and FAISS (Facebook AI Similarity Search) for efficient nearest neighbor search. The RAG model combines a retriever and a generator to perform information retrieval and generate responses.\n",
    "\n",
    "#### Pre-requisites\n",
    "\n",
    "1. **Install Required Libraries**\n",
    "   First, ensure you have the necessary libraries installed. You need `transformers`, `faiss-cpu`, `numpy` and `torch`. Install them using pip:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers faiss-cpu torch numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import RagTokenizer, RagTokenForGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Imports**: \n",
    "  - `torch` for PyTorch tensor operations.\n",
    "  - `faiss` for efficient similarity search.\n",
    "  - `numpy` for array manipulations.\n",
    "  - `RagTokenizer` and `RagTokenForGeneration` from `transformers` to use the RAG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model and Tokenizer**:\n",
    "  - `RagTokenizer` is used to preprocess the text data for the RAG model.\n",
    "  - `RagTokenForGeneration` loads the pre-trained RAG model, which includes both the retriever and generator components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some example documents\n",
    "documents = [\n",
    "    \"To reset your SmartWatch, press and hold the power button for 10 seconds until the logo appears.\",\n",
    "    \"If your SmartWatch is unresponsive, try performing a hard reset by pressing and holding the power and home buttons simultaneously.\",\n",
    "    \"Check the battery level of your SmartWatch if it is not turning on.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Documents**:\n",
    "  - A list of example documents that will be indexed and used for similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize documents\n",
    "inputs = tokenizer(documents, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tokenization**:\n",
    "  - The documents are tokenized into tensors suitable for the model. Padding and truncation ensure uniform input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate document embeddings\n",
    "with torch.no_grad():\n",
    "    # Use the encoder from the RAG model to encode the documents\n",
    "    encoder = model.rag.question_encoder\n",
    "    encoder_outputs = encoder(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    \n",
    "    # Handle tuple output\n",
    "    if isinstance(encoder_outputs, tuple):\n",
    "        encoder_outputs = encoder_outputs[0]\n",
    "    \n",
    "    # Check the shape of encoder_outputs\n",
    "    print(f\"Shape of encoder_outputs: {encoder_outputs.shape}\")\n",
    "    \n",
    "    # Ensure proper dimensions\n",
    "    if encoder_outputs.dim() == 3:\n",
    "        doc_embeddings = encoder_outputs[:, 0, :].numpy()\n",
    "    elif encoder_outputs.dim() == 2:\n",
    "        # Handle case where the output tensor is already 2D (batch_size, hidden_size)\n",
    "        doc_embeddings = encoder_outputs.numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected number of dimensions in encoder_outputs.\")\n",
    "    \n",
    "    # Ensure the array is C-contiguous\n",
    "    doc_embeddings = np.ascontiguousarray(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Embedding Generation**:\n",
    "  - Using the model’s retriever (encoder) to produce embeddings for the documents.\n",
    "  - The output is checked to ensure it’s a 3D tensor and is reshaped accordingly.\n",
    "  - The embeddings are made C-contiguous to be compatible with FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **FAISS Index Creation**:\n",
    "  - A FAISS index is created to perform efficient similarity search using the document embeddings. The index type `IndexFlatL2` uses L2 (Euclidean) distance for comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a search function\n",
    "def search(query, top_k=1):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        query_embedding = encoder(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        \n",
    "        # Handle tuple output\n",
    "        if isinstance(query_embedding, tuple):\n",
    "            query_embedding = query_embedding[0]\n",
    "        \n",
    "        # Check the shape of query_embedding\n",
    "        print(f\"Shape of query_embedding: {query_embedding.shape}\")\n",
    "        \n",
    "        # Ensure proper dimensions\n",
    "        if query_embedding.dim() == 3:\n",
    "            query_embedding = query_embedding[:, 0, :].numpy()\n",
    "        elif query_embedding.dim() == 2:\n",
    "            # Handle case where the output tensor is already 2D (batch_size, hidden_size)\n",
    "            query_embedding = query_embedding.numpy()\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected number of dimensions in query_embedding.\")\n",
    "        \n",
    "        # Ensure the array is C-contiguous\n",
    "        query_embedding = np.ascontiguousarray(query_embedding)\n",
    "    \n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Search Function**:\n",
    "  - Tokenizes the query and generates its embedding.\n",
    "  - The embedding is converted to a C-contiguous NumPy array.\n",
    "  - The FAISS index is queried to find the closest document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform search\n",
    "query = \"How do I reset my SmartWatch?\"\n",
    "if query:\n",
    "    top_indices = search(query, top_k=1)\n",
    "    top_documents = [documents[idx] for idx in top_indices]\n",
    "    print(\"Response (on Top document):\", top_documents[0])\n",
    "else:\n",
    "    print(\"Query is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Search Execution**:\n",
    "  - Performs a search with a sample query and prints the most relevant document from the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Code Example\n",
    "\n",
    "Here is the complete code snippet for semantic search using RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import RagTokenizer, RagTokenForGeneration\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\")\n",
    "\n",
    "# Define some example documents\n",
    "documents = [\n",
    "    \"To reset your SmartWatch, press and hold the power button for 10 seconds until the logo appears.\",\n",
    "    \"If your SmartWatch is unresponsive, try performing a hard reset by pressing and holding the power and home buttons simultaneously.\",\n",
    "    \"Check the battery level of your SmartWatch if it is not turning on.\"\n",
    "]\n",
    "\n",
    "# Tokenize documents\n",
    "inputs = tokenizer(documents, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Generate document embeddings\n",
    "with torch.no_grad():\n",
    "    # Use the encoder from the RAG model to encode the documents\n",
    "    encoder = model.rag.question_encoder\n",
    "    encoder_outputs = encoder(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    \n",
    "    # Handle tuple output\n",
    "    if isinstance(encoder_outputs, tuple):\n",
    "        encoder_outputs = encoder_outputs[0]\n",
    "    \n",
    "    # Check the shape of encoder_outputs\n",
    "    print(f\"Shape of encoder_outputs: {encoder_outputs.shape}\")\n",
    "    \n",
    "    # Ensure proper dimensions\n",
    "    if encoder_outputs.dim() == 3:\n",
    "        doc_embeddings = encoder_outputs[:, 0, :].numpy()\n",
    "    elif encoder_outputs.dim() == 2:\n",
    "        # Handle case where the output tensor is already 2D (batch_size, hidden_size)\n",
    "        doc_embeddings = encoder_outputs.numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected number of dimensions in encoder_outputs.\")\n",
    "    \n",
    "    # Ensure the array is C-contiguous\n",
    "    doc_embeddings = np.ascontiguousarray(doc_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Define a search function\n",
    "def search(query, top_k=1):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        query_embedding = encoder(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        \n",
    "        # Handle tuple output\n",
    "        if isinstance(query_embedding, tuple):\n",
    "            query_embedding = query_embedding[0]\n",
    "        \n",
    "        # Check the shape of query_embedding\n",
    "        print(f\"Shape of query_embedding: {query_embedding.shape}\")\n",
    "        \n",
    "        # Ensure proper dimensions\n",
    "        if query_embedding.dim() == 3:\n",
    "            query_embedding = query_embedding[:, 0, :].numpy()\n",
    "        elif query_embedding.dim() == 2:\n",
    "            # Handle case where the output tensor is already 2D (batch_size, hidden_size)\n",
    "            query_embedding = query_embedding.numpy()\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected number of dimensions in query_embedding.\")\n",
    "        \n",
    "        # Ensure the array is C-contiguous\n",
    "        query_embedding = np.ascontiguousarray(query_embedding)\n",
    "    \n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return indices[0]\n",
    "\n",
    "# Perform search\n",
    "query = \"How do I reset my SmartWatch?\"\n",
    "if query:\n",
    "    top_indices = search(query, top_k=1)\n",
    "    top_documents = [documents[idx] for idx in top_indices]\n",
    "    print(\"Response (on Top document):\", top_documents[0])\n",
    "else:\n",
    "    print(\"Query is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "This example illustrates how to use a RAG model to convert text documents into embeddings and then use FAISS for semantic search. The key steps involve tokenizing the input documents, generating embeddings using the RAG model’s encoder, creating a FAISS index, and querying this index to find the most similar documents to a given query. This setup enables efficient and scalable semantic search capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
