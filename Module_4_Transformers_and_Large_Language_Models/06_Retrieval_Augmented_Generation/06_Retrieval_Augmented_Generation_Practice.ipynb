{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Simple RAG Model\n",
    "\n",
    "1. **Setup**\n",
    "\n",
    "    First, ensure you have the necessary packages installed. You’ll need the transformers and datasets libraries, which can be installed using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Python Script for a Simple RAG Model**\n",
    "\n",
    "    This script demonstrates how to use a pre-trained RAG model to perform a question-answering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Load model and tokenizer for text generation\n",
    "def load_model_and_tokenizer():\n",
    "    model_name = \"t5-base\"  # You can change this to a different model like \"facebook/bart-large\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Generate a response using the model\n",
    "def generate_response(model, tokenizer, query):\n",
    "    # Define context\n",
    "    context = \"To reset your password, go to the settings page and click 'Reset Password'. For billing inquiries, contact our support team at billing@example.com. Our office hours are from 9 AM to 5 PM, Monday through Friday.\"\n",
    "    input_text = f\"Context: {context}\\n\\nQuery: {query}\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=150,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        temperature=1.0,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Use a question-answering pipeline with a fine-tuned model\n",
    "def answer_question(context, question):\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "# Main function to test the models\n",
    "def main():\n",
    "    # Load the text generation model\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "\n",
    "    # Define query\n",
    "    user_query = \"How can I reset my password?\"\n",
    "    \n",
    "    # Generate response using the sequence-to-sequence model\n",
    "    response = generate_response(model, tokenizer, user_query)\n",
    "    print(\"Generated Response:\", response)\n",
    "\n",
    "    # Define context for question-answering\n",
    "    context = \"To reset your password, go to the settings page and click 'Reset Password'. For billing inquiries, contact our support team at billing@example.com. Our office hours are from 9 AM to 5 PM, Monday through Friday.\"\n",
    "    \n",
    "    # Get answer using the question-answering pipeline\n",
    "    answer = answer_question(context, user_query)\n",
    "    print(\"QA Pipeline Answer:\", answer)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "This simple RAG example demonstrates how to work with text generation and question-answering models using the `transformers` library from Hugging Face. Here’s a breakdown of each part of the code and its purpose:\n",
    "\n",
    "### 1. **Import Libraries**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "```\n",
    "\n",
    "- **`AutoTokenizer`**: Automatically selects the appropriate tokenizer based on the model name.\n",
    "- **`AutoModelForSeq2SeqLM`**: Automatically loads a sequence-to-sequence model (e.g., T5, BART) for tasks like text generation.\n",
    "- **`pipeline`**: A high-level interface for various NLP tasks, including question-answering.\n",
    "\n",
    "### 2. **Load Model and Tokenizer**\n",
    "\n",
    "```python\n",
    "def load_model_and_tokenizer():\n",
    "    model_name = \"t5-base\"  # You can change this to a different model like \"facebook/bart-large\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "```\n",
    "\n",
    "- **`model_name`**: Specifies which pre-trained model to load (in this case, \"t5-base\").\n",
    "- **`AutoTokenizer.from_pretrained`**: Loads the tokenizer associated with the model. The tokenizer converts text into a format the model can understand (e.g., token IDs).\n",
    "- **`AutoModelForSeq2SeqLM.from_pretrained`**: Loads the pre-trained model itself, which can be used for text generation or other sequence-to-sequence tasks.\n",
    "\n",
    "### 3. **Generate Response**\n",
    "\n",
    "```python\n",
    "def generate_response(model, tokenizer, query):\n",
    "    # Define context\n",
    "    context = \"To reset your password, go to the settings page and click 'Reset Password'. For billing inquiries, contact our support team at billing@example.com. Our office hours are from 9 AM to 5 PM, Monday through Friday.\"\n",
    "    input_text = f\"Context: {context}\\n\\nQuery: {query}\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=150,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        temperature=1.0,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "```\n",
    "\n",
    "- **`context`**: Provides the dataset with context for possible queries.\n",
    "- **`input_text`**: Combines the context and query into a single string.\n",
    "- **`tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)`**: Tokenizes the input text and formats it as tensors (PyTorch format) for the model.\n",
    "- **`model.generate`**: Generates text based on the tokenized input. Key parameters:\n",
    "  - `max_length`: Maximum length of the generated response.\n",
    "  - `num_beams`: Number of beams for beam search (used for generating more coherent text).\n",
    "  - `early_stopping`: Stops the generation process when the model decides that it has generated a complete response.\n",
    "  - `temperature`: Controls randomness (1.0 means standard generation).\n",
    "  - `do_sample`: Indicates whether to use sampling for generating text.\n",
    "- **`tokenizer.decode`**: Converts the model output (token IDs) back into human-readable text.\n",
    "\n",
    "### 4. **Question-Answering Pipeline**\n",
    "\n",
    "```python\n",
    "def answer_question(context, question):\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "```\n",
    "\n",
    "- **`pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")`**: Initializes a question-answering pipeline using a fine-tuned model (RoBERTa in this case).\n",
    "- **`qa_pipeline(question=question, context=context)`**: Provides an answer to the question based on the given context. The model returns the most relevant answer extracted from the context.\n",
    "\n",
    "### 5. **Main Function**\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    # Load the text generation model\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "\n",
    "    # Define query\n",
    "    user_query = \"How can I reset my password?\"\n",
    "    \n",
    "    # Generate response using the sequence-to-sequence model\n",
    "    response = generate_response(model, tokenizer, user_query)\n",
    "    print(\"Generated Response:\", response)\n",
    "\n",
    "    # Define context for question-answering\n",
    "    context = \"To reset your password, go to the settings page and click 'Reset Password'. For billing inquiries, contact our support team at billing@example.com. Our office hours are from 9 AM to 5 PM, Monday through Friday.\"\n",
    "    \n",
    "    # Get answer using the question-answering pipeline\n",
    "    answer = answer_question(context, user_query)\n",
    "    print(\"QA Pipeline Answer:\", answer)\n",
    "```\n",
    "\n",
    "- **`load_model_and_tokenizer()`**: Loads the pre-trained text generation model and tokenizer.\n",
    "- **`user_query`**: The question to be answered.\n",
    "- **`generate_response(model, tokenizer, user_query)`**: Generates a response using the sequence-to-sequence model.\n",
    "- **`answer_question(context, user_query)`**: Uses the question-answering pipeline to extract an answer from the context.\n",
    "\n",
    "### 6. **Run the Main Function**\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "- Ensures that the `main()` function runs when the script is executed directly.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The script demonstrates two approaches to handling user queries:\n",
    "\n",
    "1. **Text Generation**: Uses a sequence-to-sequence model (T5 in this case) to generate responses based on a context and a query.\n",
    "2. **Question-Answering**: Uses a fine-tuned RoBERTa model to extract specific answers from a provided context.\n",
    "\n",
    "These methods highlight different strategies in NLP for responding to user queries: generating free-form text versus extracting specific information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
