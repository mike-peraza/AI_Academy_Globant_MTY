{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Architecture with PyTorch Examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Transformer Overview\n",
    "\n",
    "The Transformer model consists of an encoder and a decoder. Each of these components is made up of several layers of attention mechanisms and feed-forward networks.\n",
    "\n",
    "![Transformer architecture](https://daleonai.com/images/screen-shot-2021-05-06-at-12.12.21-pm.png)\n",
    "\n",
    "As for now we have reviewed **Self-Attention** and **Multiheaded Attention**, let's take a look on how to implement them with examples with PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Self-Attention Calculation\n",
    "\n",
    "The self-attention mechanism can be described with the following steps:\n",
    "\n",
    "1. Compute Query, Key, and Value matrices from the input.\n",
    "2. Calculate attention scores using the dot product of Query and Key.\n",
    "3. Apply a Softmax function to the attention scores.\n",
    "4. Compute the weighted sum of the Value vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### PyTorch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    dimension = torch.sqrt(torch.Tensor([K.shape[-1]]))\n",
    "    scores = torch.matmul(Q, K.transpose(-2,-1)) / dimension\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(weights, V)\n",
    "    return output\n",
    "\n",
    "# Example\n",
    "Q = torch.randn(1, 3, 4)  # (batch_size, seq_len, embed_dim)\n",
    "K = torch.randn(1, 3, 4)\n",
    "V = torch.randn(1, 3, 4)\n",
    "\n",
    "print(f'Q:{Q}\\nK:{K}\\nV:{V}')\n",
    "attention_output = self_attention(Q, K, V)\n",
    "print(f'Attention:{attention_output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Positional Encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the description of the original Transformer model, apart from the embeddings the authors provided\n",
    "a mechanism to encode positions of the word.\n",
    "\n",
    "They describe the positional encoding $PE$ in terms of cosines and sines of the positions of the words. These are not dependant\n",
    "on the embeddings. Consider $i$ ranging from $0$ to $d_{model}/2$, and $pos$ the position of the embedding vector, then the \n",
    "$PE$ matrix is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "PE(pos, 2i) &=\\sin(\\frac{pos}{l^{2i/d_{model}}}) \\\\\n",
    "PE(pos, 2i+1) &=\\cos(\\frac{pos}{l^{2i/d_{model}}})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $l$ is a user defined scalar, in the paper _Attention is all you need_ they showcase $l=10000$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, l=10000):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        positions = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(l) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(positions * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(positions * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)]\n",
    "\n",
    "# Example\n",
    "pos_encoding = PositionalEncoding(d_model=4)\n",
    "x = torch.randn(1, 3, 4)\n",
    "encoded_x = pos_encoding(x)\n",
    "print(encoded_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Encoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A Transformer encoder block consists of the following layers:\n",
    "\n",
    "- Multi-Head Attention\n",
    "- Add & Norm\n",
    "- Feed-Forward Network\n",
    "- Add & Norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x, v=None):\n",
    "        if v == None:\n",
    "            attn_output, _ = self.attention(x, x, x)\n",
    "        else:\n",
    "            attn_output, _ = self.attention(x, x, v)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "# Example\n",
    "transformer_block = TransformerBlock(d_model=4, nhead=2, dim_feedforward=8)\n",
    "x = torch.randn(3, 1, 4)  # (seq_len, batch_size, d_model)\n",
    "output = transformer_block(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, nhead, dim_feedforward)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Example\n",
    "model = TransformerEncoder(d_model=4, nhead=2, num_layers=2, dim_feedforward=8)\n",
    "x = torch.randn(3, 1, 4)  # (seq_len, batch_size, d_model)\n",
    "output = model(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "\n",
    "For the decoder, the implementation employs a **masking** mechanism in the self attention portion. To prevent illegal connections from happening, that is, for the\n",
    "self attention in the masked decoder block to only allow \"seeing\" to past portions of the sequence we can use a triangular lower matrix:\n",
    "\n",
    "$$\n",
    "\\operatorname{mask} = \\begin{bmatrix}\n",
    "1 & 0 & \\cdots & 0 \\\\\n",
    "1 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & 1 & \\cdots & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We will apply this mask to the $QK^T$ matrix making entries that don't match with the ones, $-\\infty$, so they won't contribute anything to the Softmax calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(MaskedMultiHeadAttention, self).__init__()\n",
    "        assert d_model % nhead == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.head_dim = d_model // nhead\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        def shape(x):\n",
    "            # (batch_size, seq_length, d_model) -> (batch_size, nhead, seq_length, head_dim)\n",
    "            x = x.view(batch_size, -1, self.nhead, self.head_dim)\n",
    "            return x.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Linear projections\n",
    "        q = shape(self.q_linear(query))\n",
    "        k = shape(self.k_linear(key))\n",
    "        v = shape(self.v_linear(value))\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Apply the mask to the attention scores\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Concat heads and pass through final linear layer\n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.out_linear(output)\n",
    "\n",
    "# Example usage:\n",
    "d_model = 64\n",
    "nhead = 8\n",
    "seq_length = 20\n",
    "attention = MaskedMultiHeadAttention(d_model, nhead)\n",
    "\n",
    "# Random input tensors\n",
    "query = torch.randn(1, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "key = torch.randn(1, seq_length, d_model)\n",
    "value = torch.randn(1, seq_length, d_model)\n",
    "\n",
    "# Masking: assume that mask is of shape (batch_size, nhead, seq_length, seq_length)\n",
    "mask = torch.tril(torch.ones(seq_length, seq_length)).unsqueeze(0).unsqueeze(0)  # Lower triangular matrix for causal mask\n",
    "mask = mask.to(dtype=torch.bool)  # Convert to boolean mask\n",
    "\n",
    "# Forward pass\n",
    "output = attention(query, key, value, mask)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally assemble the PyTorch implementation of the Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, mask):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.mask = mask\n",
    "        self.masked_attn = MaskedMultiHeadAttention(d_model, nhead)\n",
    "        self.transformer_block = TransformerBlock(d_model, nhead, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, output, x):\n",
    "        attn_output = self.masked_attn(output, output, output, self.mask)\n",
    "        attn_output = output + self.dropout(attn_output)\n",
    "        attn_output = self.norm(attn_output)\n",
    "        x = self.transformer_block(x, attn_output)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dim_out, mask):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = TransformerEncoder(d_model, nhead, num_layers, dim_feedforward)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.decoder = nn.ModuleList([DecoderBlock(d_model, nhead, dim_feedforward, mask)\n",
    "                                          for _ in range(num_layers)])\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_out),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        out = self.positional_encoding(x)\n",
    "        for layer in self.decoder:\n",
    "            out = layer(out, x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "# Example\n",
    "d_model = 64\n",
    "nhead = 8\n",
    "seq_length = 20\n",
    "attention = MaskedMultiHeadAttention(d_model, nhead)\n",
    "\n",
    "# Random input tensors\n",
    "x = torch.randn(1, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "\n",
    "# Masking: assume that mask is of shape (batch_size, nhead, seq_length, seq_length)\n",
    "mask = torch.tril(torch.ones(seq_length, seq_length)).unsqueeze(0).unsqueeze(0)  # Lower triangular matrix for causal mask\n",
    "mask = mask.to(dtype=torch.bool)  # Convert to boolean mask\n",
    "\n",
    "model = Transformer(d_model=d_model, nhead=nhead, num_layers=2, dim_feedforward=8, dim_out=12, mask=mask)\n",
    "output = model(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "In this notebook, we have explored the core components of the Transformer architecture, including self-attention, positional encoding, and the structure of Transformer blocks. We have also provided PyTorch implementations for each of these components.\n",
    "\n",
    "Feel free to experiment with different configurations and datasets to better understand how Transformers work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "[Grant Sanderson on Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
