{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Architecture with PyTorch Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Transformer Overview\n",
    "\n",
    "The Transformer model consists of an encoder and a decoder. Each of these components is made up of several layers of attention mechanisms and feed-forward networks.\n",
    "\n",
    "![Transformer architecture](https://daleonai.com/images/screen-shot-2021-05-06-at-12.12.21-pm.png)\n",
    "\n",
    "As for now we have reviewed **Self-Attention** and **Multiheaded Attention**, let's take a look on how to implement them with examples with PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Self-Attention Calculation\n",
    "\n",
    "The self-attention mechanism can be described with the following steps:\n",
    "\n",
    "1. Compute Query, Key, and Value matrices from the input.\n",
    "2. Calculate attention scores using the dot product of Query and Key.\n",
    "3. Apply a Softmax function to the attention scores.\n",
    "4. Compute the weighted sum of the Value vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### PyTorch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:tensor([[[ 1.6851,  0.1665, -0.0917, -0.2914],\n",
      "         [-0.8735,  0.8515,  1.1608,  0.1396],\n",
      "         [-0.1546,  1.5666,  0.9541,  1.1648]]])\n",
      "K:tensor([[[ 0.4788,  0.3367,  1.1149, -0.2138],\n",
      "         [ 1.7596, -0.5329,  0.6616, -1.7132],\n",
      "         [ 0.3233,  0.9728,  0.9566, -0.4169]]])\n",
      "V:tensor([[[ 1.0031,  0.5750, -1.0804,  1.0226],\n",
      "         [-2.3141,  0.3248, -0.1342, -0.5509],\n",
      "         [-0.1072,  1.4614, -0.6232, -0.3182]]])\n",
      "Attention:tensor([[[-1.3144,  0.5714, -0.3945, -0.2204],\n",
      "         [ 0.0929,  0.9892, -0.7508,  0.1856],\n",
      "         [ 0.1976,  1.0412, -0.7743,  0.1983]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    dimension = torch.sqrt(torch.Tensor([K.shape[-1]]))\n",
    "    scores = torch.matmul(Q, K.transpose(-2,-1)) / dimension\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(weights, V)\n",
    "    return output\n",
    "\n",
    "# Example\n",
    "Q = torch.randn(1, 3, 4)  # (batch_size, seq_len, embed_dim)\n",
    "K = torch.randn(1, 3, 4)\n",
    "V = torch.randn(1, 3, 4)\n",
    "\n",
    "print(f'Q:{Q}\\nK:{K}\\nV:{V}')\n",
    "attention_output = self_attention(Q, K, V)\n",
    "print(f'Attention:{attention_output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Positional Encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "In the description of the original Transformer model , \n",
    "\n",
    "The positional encoding for a given position pos $pos$ and dimension $i$ is given by:\n",
    "\n",
    "$$PE(pos,2i)=sin⁡(pos/100002i/d)PE(pos,2i)=sin(pos/100002i/d) PE(pos,2i+1)=cos⁡(pos/100002i/d)PE(pos,2i+1)=cos(pos/100002i/d)\n",
    "\n",
    "where dd is the dimensionality of the embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyTorch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        positions = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(positions * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(positions * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)]\n",
    "\n",
    "# Example\n",
    "pos_encoding = PositionalEncoding(d_model=4)\n",
    "x = torch.randn(1, 3, 4)\n",
    "encoded_x = pos_encoding(x)\n",
    "print(encoded_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Transformer Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A Transformer block consists of the following layers:\n",
    "\n",
    "    Multi-Head Attention\n",
    "    Add & Norm\n",
    "    Feed-Forward Network\n",
    "    Add & Norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PyTorch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "python\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "# Example\n",
    "transformer_block = TransformerBlock(d_model=4, nhead=2, dim_feedforward=8)\n",
    "x = torch.randn(3, 1, 4)  # (seq_len, batch_size, d_model)\n",
    "output = transformer_block(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Multi-Head Attention allows the model to focus on different parts of the sequence from multiple perspectives. It involves multiple self-attention mechanisms in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyTorch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "python\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead)\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.attention(query, key, value)\n",
    "        return attn_output\n",
    "\n",
    "# Example\n",
    "multi_head_attention = MultiHeadAttention(d_model=4, nhead=2)\n",
    "x = torch.randn(3, 1, 4)  # (seq_len, batch_size, d_model)\n",
    "output = multi_head_attention(x, x, x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Feed-Forward Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Each Transformer block contains a feed-forward network that consists of two linear transformations with a ReLU activation in between.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyTorch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "python\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)\n",
    "\n",
    "# Example\n",
    "feed_forward = FeedForwardNetwork(d_model=4, dim_feedforward=8)\n",
    "x = torch.randn(3, 1, 4)  # (seq_len, batch_size, d_model)\n",
    "output = feed_forward(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Full Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A full Transformer model combines multiple Transformer blocks. The encoder and decoder stacks can be constructed by stacking these blocks.\n",
    "PyTorch Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "python\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, nhead, dim_feedforward)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Example\n",
    "model = TransformerModel(d_model=4, nhead=2, num_layers=2, dim_feedforward=8)\n",
    "x = torch.randn(3, 1, 4)  # (seq_len, batch_size, d_model)\n",
    "output = model(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conclusion\n",
    "\n",
    "In this notebook, we have explored the core components of the Transformer architecture, including self-attention, positional encoding, and the structure of Transformer blocks. We have also provided PyTorch implementations for each of these components.\n",
    "\n",
    "Feel free to experiment with different configurations and datasets to better understand how Transformers work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "[Grant Sanderson on Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
