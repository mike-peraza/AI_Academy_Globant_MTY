{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing sentiments from text data\n",
    "\n",
    "Library Installation:\n",
    "- You need to install the necessary libraries using pip in a terminal window.\n",
    "Specifically, install scikit-learn.\n",
    "- Assumption: You already have pandas and numpy installed. If not, install them using pip.\n",
    "\n",
    "Sentiment Analysis Comparison:\n",
    "The exercise involves analyzing sentiments using two different approaches: Word2Vec and TF-IDF.\n",
    "\n",
    "Each piece of code will be marked to indicate which approach it corresponds to.\n",
    "\n",
    "In summary, you’ll install libraries, perform sentiment analysis, and compare the results between Word2Vec and TF-IDF.\n",
    "\n",
    "Word2Vec:\n",
    "- Purpose: Word2Vec converts words into dense, fixed-size vectors (embeddings) in an n-dimensional space.\n",
    "- Method: It learns these embeddings by analyzing the context in which words appear within a large text corpus.\n",
    "- Benefits: \n",
    "    - Captures semantic relationships between words (e.g., ‘house’ and ‘home’ are similar).\n",
    "    - Enables understanding of word meanings.\n",
    "    - Useful for downstream tasks like sentiment analysis and recommendation systems.\n",
    "- Example: The word ‘car’ might be represented as [-0.016, -0.0003, 0.0899, …].\n",
    "- Use Case: Great for deep learning models.\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "- Purpose: TF-IDF converts text documents into sparse vectors.\n",
    "- Method: It considers word frequencies in each document relative to their occurrence across all documents.\n",
    "- Benefits:\n",
    "    - Reflects word importance within a specific document.\n",
    "    - Commonly used for text classification and information retrieval.\n",
    "- Example: If a document contains ‘house,’ the ‘house’ column has a ‘1’ (non-zero value).\n",
    "- Use Case: Simple machine learning algorithms.\n",
    "\n",
    "In summary, both methods offer sophisticated ways to represent language numerically. Word2Vec captures context and semantics, while TF-IDF focuses on word importance within documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Parameter 06"
    ]
   },
   "outputs": [],
   "source": [
    "#libraries used only for Word2Vec \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#libraries used for both\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset for Word2Vec\n",
    "df = pd.read_csv('sentiments.csv')\n",
    "\n",
    "# Download nltk data (if not already downloaded) for Word2Vec\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Function to pre-Procesing text for Word2Vec\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Tokenize and preprocess text for Word2Vec\n",
    "df['tokens'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Train Word2Vec model (Parameters 01)\n",
    "word2vec_model = Word2Vec(df['tokens'], vector_size=200, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Function to average word vectors for Word2Vec\n",
    "def average_word_vectors(tokens, model, vector_size):\n",
    "    if len(tokens) < 1:\n",
    "        return np.zeros(vector_size)\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) < 1:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Vectorize text using averaged Word2Vec\n",
    "df['word_vectors'] = df['tokens'].apply(lambda x: average_word_vectors(x, word2vec_model, 200))\n",
    "X_word2vec = pd.DataFrame(df['word_vectors'].tolist())\n",
    "y = df['sentiment']\n",
    "\n",
    "# TF-IDF Vectorization on Word2Vec\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=200)\n",
    "# TF-IDF Vectorization on TF-IDF\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['text']).toarray()\n",
    "\n",
    "# Feature Scaling declaration\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Feature Scaling on Word2Vec\n",
    "X_word2vec_scaled = scaler.fit_transform(X_word2vec)\n",
    "# Feature Scaling on TF-IDF\n",
    "X_tfidf_scaled = scaler.fit_transform(X_tfidf)\n",
    "\n",
    "# Splitting data into training and testing sets on Word2Vec (Parameters 02)\n",
    "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(X_word2vec_scaled, y, test_size=0.2, random_state=42)\n",
    "# Splitting data into training and testing sets on TF-IDF (Parameters 03)\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(X_tfidf_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model on Word2Vec (Parameters 04)\n",
    "model_w2v = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Train Logistic Regression model on TF-IDF (Parameters 05)\n",
    "model_tfidf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predicting on test data Word2Vec\n",
    "y_pred_w2v = model_w2v.predict(X_test_w2v)\n",
    "# Predicting on test data TF-IDF\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Word2Vec Model Evaluation\")\n",
    "print(f\"Train Accuracy: {accuracy_score(y_train, model_w2v.predict(X_train_w2v))}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_w2v)}\")\n",
    "print(classification_report(y_test, y_pred_w2v, zero_division=1))\n",
    "\n",
    "print(\"TF-IDF Model Evaluation\")\n",
    "print(f\"Train Accuracy: {accuracy_score(y_train, model_tfidf.predict(X_train_tfidf))}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_tfidf)}\")\n",
    "print(classification_report(y_test, y_pred_tfidf, zero_division=1))\n",
    "\n",
    "# Function to predict sentiment of new text using Word2Vec\n",
    "def predict_sentiment_w2v(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    word_vector = average_word_vectors(tokens, word2vec_model, 200) #Parameters 06\n",
    "    word_vector_scaled = scaler.transform([word_vector])\n",
    "    return model_w2v.predict(word_vector_scaled)[0]\n",
    "\n",
    "# Function to predict sentiment of new text using TF-IDF\n",
    "def predict_sentiment_tfidf(text):\n",
    "    text_transformed = tfidf_vectorizer.transform([text]).toarray()\n",
    "    text_transformed_scaled = scaler.transform(text_transformed)\n",
    "    return model_tfidf.predict(text_transformed_scaled)[0]\n",
    "\n",
    "# Example usage\n",
    "new_text_1 = \"I am very satisfied with the service.\"\n",
    "print('Sentiment for phrase: ', new_text_1)\n",
    "print(f\"Word2Vec Sentiment: {predict_sentiment_w2v(new_text_1)}\")\n",
    "print(f\"TF-IDF Sentiment: {predict_sentiment_tfidf(new_text_1)}\")\n",
    "\n",
    "new_text_2 = \"Terrible, would not recommend.\"\n",
    "print('Sentiment for phrase: ', new_text_2)\n",
    "print(f\"Word2Vec Sentiment: {predict_sentiment_w2v(new_text_2)}\")\n",
    "print(f\"TF-IDF Sentiment: {predict_sentiment_tfidf(new_text_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s break down the parameters (feel free to play with them)\n",
    "\n",
    "Parameters 01 (Word2Vec(df['tokens'], vector_size=200, window=5, min_count=1, sg=1))\n",
    "- df['tokens']: is the loaded file and converted to DataFrame (df) with a column named 'tokens'. The 'tokens' column likely contains preprocessed text data (such as tokenized words).\n",
    "- vector_size=200: This parameter specifies the dimensionality of the word vectors (also known as word embeddings). In this case, each word will be represented as a 200-dimensional vector.\n",
    "- window=5: The window parameter determines the maximum distance between the current word and the context words within a sentence. A smaller window focuses on nearby context words, while a larger window considers a broader context.\n",
    "- min_count=1: This parameter sets the minimum frequency count for a word to be included in the vocabulary. Words that occur less frequently than the specified count are ignored.\n",
    "- sg=1: The sg parameter stands for “skip-gram.” When sg=1, the skip-gram model is used; when sg=0, the continuous bag-of-words (CBOW) model is used. Skip-gram aims to predict context words given a target word, while CBOW predicts the target word based on context.\n",
    "In summary, the Word2Vec function trains word embeddings based on the provided text data, capturing semantic relationships between words. The resulting vectors can be used for various natural language processing tasks\n",
    "\n",
    "Parameters 02 & 03 (train_test_split(X_word2vec_scaled, y, test_size=0.2, random_state=42))\n",
    "- X_word2vec_scaled: This is the feature matrix (often denoted as X) containing the input data (independent variables). It represents the features you’ll use to train your machine learning model.\n",
    "- y: This is the target vector (often denoted as y) containing the output labels (dependent variable). It represents the values you’re trying to predict or classify.\n",
    "- test_size=0.2: This parameter specifies the proportion of the dataset that should be allocated to the test set. In this case, 20% of the data will be used for testing, while the remaining 80% will be used for training.\n",
    "- random_state=42: This parameter sets the random seed for shuffling the data before splitting. It ensures that the same split is obtained each time you run the code with the same random seed (useful for reproducibility).\n",
    "In summary, train_test_split divides your data into training and test sets, allowing you to evaluate your model’s performance on unseen data. The split ensures that your evaluation is unbiased. \n",
    "\n",
    "Parameters 04 & 05 (LogisticRegression(max_iter=1000, random_state=42))\n",
    "- max_iter=1000: This parameter specifies the maximum number of iterations for the optimization algorithm during training. It determines how many times the algorithm updates the model’s coefficients to find the best fit. In this case, the maximum number of iterations is set to 1000.\n",
    "- random_state=42: The random_state parameter sets the random seed for initializing the model’s internal random number generator. It ensures that the same random initialization is used each time you run the code with the same random seed (useful for reproducibility).\n",
    "In summary, the LogisticRegression model is a popular classification algorithm used for binary and multiclass classification tasks. It optimizes the coefficients to fit the data and make predictions based on logistic regression\n",
    "\n",
    "Parameters 06 (average_word_vectors(tokens, word2vec_model, 200))\n",
    "- tokens: This parameter likely represents a list of tokenized words or phrases (e.g., individual words or n-grams) from your text data.\n",
    "- word2vec_model: This refers to the Word2Vec model that you’ve trained earlier. It contains word embeddings (vectors) learned from your text corpus.\n",
    "- 200: This value specifies the dimensionality of the word vectors. In this case, each word vector is expected to have 200 dimensions.\n",
    "The purpose of the average_word_vectors function is likely to compute the average vector representation for a given set of tokens. Here’s how it might work:\n",
    "- For each token in the tokens list:\n",
    "    - Look up its corresponding word vector in the word2vec_model.\n",
    "    - Sum up all the word vectors.\n",
    "    - Divide the sum by the total number of tokens to get the average vector.\n",
    "The resulting average vector can be used as a representation for the entire set of tokens. This approach is often used when you want to represent a document or sentence as a single vector based on its constituent words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
