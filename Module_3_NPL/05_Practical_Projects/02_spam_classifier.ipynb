{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a spam classifier using NLP techniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Dataset Preparation\n",
    "##### 1.- Load the Dataset: \n",
    "Use a dataset that includes text messages labeled as spam or not spam. Common datasets include the SMS Spam Collection from UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"SMSSpamCollection\", sep='\\t', header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.- Explore the Dataset: \n",
    "Understand the structure and content of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Data Preprocessing\n",
    "##### 1.- Text Cleaning: \n",
    "Remove unnecessary characters, punctuation, and stopwords (common words like 'and', 'the')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    text = ' '.join([word for word in word_tokenize(text) if word not in stopwords.words('english')])  # Remove stopwords\n",
    "    return text\n",
    "    \n",
    "df['clean_text'] = df['message'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 02.- Feature Extraction: \n",
    "Convert text data into numerical vectors using techniques like TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Train-Test Split\n",
    "##### Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Model Training and Evaluation\n",
    "##### 1.- Choose a Classifier: \n",
    "Train a classifier, such as a Naive Bayes classifier, which often works well for text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9695067264573991\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98       966\n",
      "        spam       1.00      0.77      0.87       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.89      0.93      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Deployment\n",
    "Once satisfied with the model's performance, you can save the trained model and use it to predict on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the trained model for prediction\n",
    "new_texts = [\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\"]\n",
    "new_texts_clean = [clean_text(text) for text in new_texts]\n",
    "new_texts_tfidf = tfidf_vectorizer.transform(new_texts_clean)\n",
    "predictions = clf.predict(new_texts_tfidf)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code complete in a single file\n",
    "lets see the code in a single file and the behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.- \n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"SMSSpamCollection\", sep='\\t', header=None, names=['label', 'message'])\n",
    "\n",
    "#2.- \n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    text = ' '.join([word for word in word_tokenize(text) if word not in stopwords.words('english')])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['message'].apply(clean_text)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
    "y = df['label']\n",
    "\n",
    "#3.-\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#4.- \n",
    "# Initialize the classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#5.- \n",
    "# Example of how to use the trained model for prediction\n",
    "new_texts = [\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\"]\n",
    "new_texts_clean = [clean_text(text) for text in new_texts]\n",
    "new_texts_tfidf = tfidf_vectorizer.transform(new_texts_clean)\n",
    "predictions = clf.predict(new_texts_tfidf)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "As you can see, we use a file (SMSSpamCollection.csv) with comments to help train AI. In this case, it’s labeled as either ‘Spam’ or ‘Ham.’ However, you can create any kind of list to train your own version. For example, the ‘enron_spam_data.csv’ is a larger dataset of emails that isn’t marked. You can use it to train and experiment. For instance, you could detect who the emails are mostly sent and received by, or explore the topics within these emails. The possibilities are limitless—enjoy exploring!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
