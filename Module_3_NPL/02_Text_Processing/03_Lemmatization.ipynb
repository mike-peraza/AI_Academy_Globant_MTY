{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a process in Natural Language Processing (NLP) that aims to reduce words to their base or dictionary form, called a lemma. \n",
    "\n",
    "Unlike stemming, which simply removes affixes from words, lemmatization considers the context and morphological analysis to generate the appropriate lemma for a word. \n",
    "\n",
    "The resulting lemmas are valid words that convey the intended meaning.\n",
    "\n",
    "Lemmatization Example: Let's consider a sentence: \"The dogs are barking loudly.\"\n",
    "\n",
    "If we apply lemmatization to this sentence, we can use the WordNet lemmatizer provided by the NLTK library in Python. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: this code can't be runned here, is required a terminal.\n",
    "# For this example, we need to download another compliment as we saw in chapter 01_Tokenization for nltk called: wordnet, jut open a terminal and write: \n",
    "python #Call pyton interpreter\n",
    "import nltk #Call the library nltk\n",
    "nltk.download('wordnet') #Download the necessary resource files\n",
    "\n",
    "# These steps will initiate the download of the necessary resource files. \n",
    "# After the download is complete, you can proceed with running your code that involves lemmatization using the WordNet lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'dog', 'are', 'barking', 'loudly', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"The dogs are barking loudly.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to each token\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Print the lemmas\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we first import the necessary NLTK modules: WordNetLemmatizer for lemmatization and word_tokenize for tokenization. \n",
    "The sentence \"The dogs are barking loudly.\" is then tokenized into separate words.\n",
    "\n",
    "Next, we initialize a WordNetLemmatizer object, which utilizes WordNet, a lexical database for English. \n",
    "We iterate over each token and apply lemmatization using the lemmatize() method of the lemmatizer object.\n",
    "\n",
    "Finally, we print the resulting lemmas, which will be ['The', 'dog', 'are', 'barking', 'loudly', '.']. \n",
    "Notice how words like \"dogs\" and \"barking\" have been reduced to their base forms \"dog\" and \"bark\", respectively.\n",
    "\n",
    "Lemmatization helps to obtain meaningful lemmas that are grammatically correct and provide a normalized representation of words. \n",
    "It is particularly useful in tasks like text analysis, information retrieval, and language generation, where precise word forms and their meanings are essential."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
