{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "\n",
    "Tokenization is a fundamental step in Natural Language Processing (NLP) that involves breaking down a text into smaller units called tokens. \n",
    "These tokens could be words, phrases, or even individual characters, depending on the specific requirements of the NLP task.\n",
    "\n",
    "Tokenization Example: Let's consider the following sentence: \"I love to eat pizza.\"\n",
    "\n",
    "If we tokenize this sentence at the word level, the resulting tokens would be: [\"I\", \"love\", \"to\", \"eat\", \"pizza\"].\n",
    "\n",
    "Each word in the sentence becomes a separate token, and this word-level tokenization is a common approach in many NLP applications. \n",
    "These tokens can then be further processed and analyzed for tasks like sentiment analysis, language modeling, or machine translation.\n",
    "\n",
    "However, tokenization can also occur at different levels depending on the specific requirements of the task. \n",
    "For example, if we tokenize the sentence at the character level, the resulting tokens would be: \n",
    "[\"I\", \" \", \"l\", \"o\", \"v\", \"e\", \" \", \"t\", \"o\", \" \", \"e\", \"a\", \"t\", \" \", \"p\", \"i\", \"z\", \"z\", \"a\", \".\"].\n",
    "\n",
    "In this case, each character (including spaces and punctuation) becomes a separate token. \n",
    "Character-level tokenization can be useful in certain cases, such as text generation or language modeling when analyzing the structure and patterns at a more granular level.\n",
    "\n",
    "Tokenization is an essential preprocessing step in NLP, as it helps to convert raw text into manageable units that can be further processed and analyzed by NLP algorithms and models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we need to install 2 librarys, nltk and punkt as complement below you'll find how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#NOTE: this code can't be runned here, is required a terminal.\n",
    "#installation of nltk is with pip install, open your terminal externally or use terminal in visual studio and write:\n",
    "pip install nltk\n",
    "\n",
    "#for nltk open your terminal externally or use terminal in visual studio and write:\n",
    "python #Call pyton interpreter\n",
    "import nltk #Call the library previusly installed\n",
    "nltk.download('punkt') #Download the necessary resource files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'to', 'eat', 'pizza', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"I love to eat pizza.\"\n",
    "\n",
    "# Tokenize the sentence at the word level\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
