{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5fd292c6",
      "metadata": {},
      "source": [
        "## Fundamentals of Deep Learning\n",
        "\n",
        "This section would cover the basic concepts of Deep Learning, including an introduction to neural networks and how data propagates forward through them.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b0847b8a",
      "metadata": {},
      "source": [
        "The father of the concept of Deep Learning was the British scientist Geoffrey Hinton. He conducted research on Deep Learning in the 1980s and currently works at Google. Many of the concepts and terminology we will discuss originate from Geoffrey Hinton's work. The main idea behind Deep Learning is to observe the human brain and draw inspiration from it to try to replicate its behavior computationally. Therefore, if our goal is to mimic the human brain, we need to bring certain elements of neuroscience into the computer. Let's see how we can achieve this."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cdc2250a",
      "metadata": {},
      "source": [
        "Deep Learning is a subfield of machine learning that focuses on training artificial neural networks to learn and make predictions from data. In this section, we would explore the foundational principles of Deep Learning:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae48fc24",
      "metadata": {},
      "source": [
        "\n",
        "1. **Neural Networks:** We would begin by introducing the concept of artificial neural networks (ANNs). ANNs are composed of interconnected nodes called neurons, organized in layers. Each neuron receives inputs, applies an activation function, and passes the output to the next layer. We would discuss the structure and architecture of neural networks.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0e6ab6dc",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"./images/Colored_neural_network.svg.png\" alt=\"neural network\" width=\"500\">\n",
        "</p>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1ec6a704",
      "metadata": {},
      "source": [
        "2. **Forward Propagation:** Next, we would delve into the process of forward propagation, also known as feedforward. This is the mechanism by which data flows through the network from the input layer to the output layer. Each neuron receives inputs, performs computations, and passes the output to the next layer until the final output is produced. We would cover how the weights and biases of the neurons are used to compute the output."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4555e0f7",
      "metadata": {},
      "source": [
        "3. **Activation Functions:** Activation functions play a crucial role in neural networks by introducing non-linearity, enabling the network to learn complex patterns. We would explore popular activation functions such as ReLU (Rectified Linear Unit), Sigmoid, and Tanh. We would discuss their characteristics, advantages, and use cases."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f5d6888e",
      "metadata": {},
      "source": [
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"./images/sample-activation-functions-square.png\" alt=\"neural network\" width=\"500\">\n",
        "</p>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d54efb2a",
      "metadata": {},
      "source": [
        "4. **Training Neural Networks:** Training neural networks involves adjusting the weights and biases of the neurons to minimize the difference between the predicted output and the actual output. We would touch upon concepts such as loss functions, optimization algorithms (e.g., gradient descent), and backpropagation. Backpropagation computes the gradients of the network's parameters with respect to the loss, allowing for iterative updates to improve the network's performance."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b4d74135",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "5. **Loss Functions:** Loss functions quantify the difference between the predicted output of a neural network and the actual output. They serve as a measure of how well the network is performing. Common loss functions include mean squared error (MSE) for regression problems and categorical cross-entropy for classification tasks. We would discuss the importance of selecting an appropriate loss function based on the problem at hand."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c996a08f",
      "metadata": {},
      "source": [
        "6. **Gradient Descent:** Gradient descent is an optimization algorithm used to update the weights and biases of a neural network during training. It operates by iteratively adjusting the parameters in the direction of steepest descent of the loss function. We would explore gradient descent variants, such as stochastic gradient descent (SGD), mini-batch gradient descent, and adaptive optimization methods (e.g., Adam, RMSprop).\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ef2db85f",
      "metadata": {},
      "source": [
        "\n",
        "7. **Overfitting and Underfitting:** Overfitting occurs when a neural network performs well on the training data but fails to generalize to new, unseen data. Underfitting, on the other hand, happens when the network fails to capture the patterns in the training data. We would discuss techniques to mitigate overfitting and underfitting, such as regularization (e.g., L1 and L2 regularization), early stopping, and dropout.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4ee08190",
      "metadata": {},
      "source": [
        "8. **Validation and Evaluation:** To assess the performance of a trained model, we need to validate and evaluate it using separate datasets. We would cover concepts such as training, validation, and test sets. Cross-validation techniques, such as k-fold cross-validation, would be introduced to obtain a more reliable estimate of the model's generalization ability.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "605a6772",
      "metadata": {},
      "source": [
        "9. **Hyperparameter Tuning:** Deep learning models have hyperparameters that need to be set manually before training. These include the learning rate, the number of layers and neurons, the activation functions, and regularization parameters. We would discuss the importance of hyperparameter tuning and techniques such as grid search and random search to find optimal hyperparameters.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b5a99f12",
      "metadata": {},
      "source": [
        "10. **Model Selection and Saving:** Once we have trained and evaluated multiple models with different hyperparameters, we need to select the best-performing model for deployment. We would discuss criteria for model selection, such as accuracy, precision, recall, and F1 score. Additionally, we would explore techniques for saving and loading trained models for future use.\n",
        "\n",
        "By covering these additional aspects of fundamentals, we would gain a comprehensive understanding of the key concepts and techniques that form the foundation of Deep Learning."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Networks: Architecture and Forward Propagation\n",
        "\n",
        "Neural networks are the foundation of deep learning. In this topic, we will explore the architecture of neural networks and understand how data propagates forward through them.\n",
        "\n",
        "## Neural Network Architecture\n",
        "\n",
        "Neural networks consist of interconnected nodes called neurons, organized in layers. The three main types of layers are input, hidden, and output layers.\n",
        "\n",
        "1. **Input Layer:**\n",
        "   - The input layer receives the raw input data, which could be images, text, or any other form of data.\n",
        "   - Each neuron in the input layer represents a feature or attribute of the input.\n",
        "\n",
        "2. **Hidden Layers:**\n",
        "   - Hidden layers are layers between the input and output layers.\n",
        "   - They perform complex computations and extract relevant features from the input data.\n",
        "   - Deep neural networks have multiple hidden layers, allowing for more intricate learning.\n",
        "\n",
        "3. **Output Layer:**\n",
        "   - The output layer produces the final predictions or outputs of the model.\n",
        "   - The number of neurons in the output layer depends on the problem type.\n",
        "   - For example, in a binary classification task, there will be one neuron representing each class.\n",
        "\n",
        "## Forward Propagation\n",
        "\n",
        "Forward propagation, also known as feedforward, is the process by which data flows through the neural network from the input layer to the output layer.\n",
        "\n",
        "1. **Weighted Sum:**\n",
        "   - Each neuron in a layer receives inputs from the previous layer, multiplies them by corresponding weights, and sums them up.\n",
        "   - This step represents the linear transformation of the data.\n",
        "   - For example, consider a neuron in the hidden layer that receives inputs x1, x2, and x3 from the previous layer. The weighted sum can be calculated as follows: `weighted_sum = (w1 * x1) + (w2 * x2) + (w3 * x3) + bias`, where w1, w2, w3 are the weights and bias represents an additional learnable parameter.\n",
        "\n",
        "2. **Activation Function:**\n",
        "   - After the weighted sum, an activation function is applied to introduce non-linearity and determine the output of each neuron.\n",
        "   - Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
        "   - For example, the ReLU activation function returns the input if it is positive and 0 otherwise. It can be defined as follows: `output = max(0, weighted_sum)`.\n",
        "\n",
        "3. **Output Calculation:**\n",
        "   - The outputs from the activation functions in the previous layer serve as inputs to the next layer.\n",
        "   - This process continues until the output layer is reached, and the final predictions are obtained.\n",
        "   - For example, in a binary classification task, the output layer might use a sigmoid activation function to produce a probability value between 0 and 1, representing the likelihood of belonging to a particular class.\n",
        "\n",
        "By understanding the architecture of neural networks and the process of forward propagation, we can effectively design and train deep learning models for a wide range of tasks.\n",
        "\n",
        "## Example\n",
        "\n",
        "Let's consider a simple example of a feedforward neural network for image classification:\n",
        "\n",
        "- Input Layer: The input layer consists of neurons that represent the pixels of an image. Each pixel value serves as a feature for the network.\n",
        "- Hidden Layers: The hidden layers perform computations and extract relevant features from the image. Each neuron in the hidden layers receives inputs from the previous layer and applies a weighted sum and activation function.\n",
        "- Output Layer: The output layer produces the final predictions, representing the probabilities of the input image belonging to different classes (e.g., cat, dog).\n",
        "\n",
        "During forward propagation, the input image is passed through the network. Each neuron in the hidden layers receives inputs from the previous layer, applies a weighted sum, and passes the result through an activation function. The output layer then produces the final predictions.\n",
        "\n",
        "This process of forward propagation allows the neural network to transform the input image and make predictions based on the learned weights and biases.\n",
        "\n",
        "By experimenting with different architectures and adjusting the weights and biases during the training process, neural networks can learn to recognize patterns and make accurate predictions on various tasks, including computer vision, natural language processing, and more.\n",
        "\n",
        "Remember to adjust the architecture and activation functions based on the specific problem you are working on. Experimentation and fine-tuning are keys to achieving optimal performance.\n",
        "\n",
        "By understanding the architecture of neural networks and the process of forward propagation, you are now equipped to design and train deep learning models for a wide range of tasks.\n",
        "\n",
        "Happy learning and exploring the fascinating world of neural networks!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4bf09253",
      "metadata": {},
      "source": [
        "## Activation Functions: ReLU, Sigmoid, Tanh\n",
        "\n",
        "Activation functions play a crucial role in neural networks by introducing non-linearity and determining the output of each neuron. Let's explore three commonly used activation functions: ReLU, Sigmoid, and Tanh.\n",
        "\n",
        "1. **ReLU (Rectified Linear Unit):**\n",
        "   - ReLU is one of the most widely used activation functions.\n",
        "   - It returns the input value if it is positive, and 0 otherwise.\n",
        "   - The ReLU function can be defined as follows: `output = max(0, weighted_sum)`.\n",
        "   - ReLU is computationally efficient and helps with sparse activation, making it particularly useful in deep neural networks.\n",
        "\n",
        "2. **Sigmoid:**\n",
        "   - The sigmoid activation function maps the input to a value between 0 and 1, producing a probability-like output.\n",
        "   - It is often used for binary classification tasks.\n",
        "   - The sigmoid function can be defined as follows: `output = 1 / (1 + exp(-weighted_sum))`.\n",
        "   - Sigmoid squashes the input to a limited range, which can lead to vanishing gradients in deep networks.\n",
        "\n",
        "3. **Tanh (Hyperbolic Tangent):**\n",
        "   - The tanh activation function is similar to the sigmoid function but maps the input to a value between -1 and 1.\n",
        "   - It is symmetric around the origin, which can be advantageous in certain scenarios.\n",
        "   - The tanh function can be defined as follows: `output = (exp(weighted_sum) - exp(-weighted_sum)) / (exp(weighted_sum) + exp(-weighted_sum))`.\n",
        "   - Tanh can help with centering the data around zero, allowing the network to learn both positive and negative weights.\n",
        "\n",
        "These activation functions determine the output of each neuron in a neural network and introduce non-linearity, which is crucial for the network to learn complex patterns and make accurate predictions. Choosing the right activation function depends on the specific problem and the desired behavior of the network.\n",
        "\n",
        "Experimenting with different activation functions and architectures is essential to achieve optimal performance in neural network models.\n",
        "\n",
        "Remember to select and apply the appropriate activation function for each layer based on the characteristics of your data and the requirements of your task.\n",
        "\n",
        "By understanding the properties and characteristics of activation functions like ReLU, Sigmoid, and Tanh, you now have the knowledge to make informed decisions when designing and training neural networks for various applications.\n",
        "\n",
        "Happy experimenting and exploring the power of activation functions in your neural network models!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b541a74d",
      "metadata": {},
      "source": [
        "## Convolutional Neural Networks (CNNs)\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are a specialized type of neural network commonly used in computer vision tasks. They are well-suited for tasks that involve analyzing and understanding visual data, such as image classification, object detection, and image segmentation.\n",
        "\n",
        "### CNN Architecture\n",
        "\n",
        "The architecture of a CNN is designed to effectively process and extract features from images. It typically consists of the following key components:\n",
        "\n",
        "1. **Convolutional Layers:**\n",
        "   - Convolutional layers are the core building blocks of a CNN.\n",
        "   - They apply a set of learnable filters (also known as kernels) to the input image, which helps in detecting visual patterns and features.\n",
        "   - Each filter performs a convolution operation by sliding across the input image, extracting local features at each position.\n",
        "\n",
        "2. **Pooling Layers:**\n",
        "   - Pooling layers follow convolutional layers and downsample the feature maps.\n",
        "   - They reduce the spatial dimensions of the input, which helps in reducing the computational complexity and controlling overfitting.\n",
        "   - Common pooling operations include max pooling and average pooling.\n",
        "\n",
        "3. **Fully Connected Layers:**\n",
        "   - Fully connected layers are traditional neural network layers that connect all neurons from the previous layer to the next.\n",
        "   - They take the high-level features extracted by convolutional and pooling layers and use them for classification or regression.\n",
        "   - The output of the last fully connected layer is connected to the output layer, which produces the final predictions.\n",
        "\n",
        "### Applications of CNNs in Computer Vision\n",
        "\n",
        "CNNs have revolutionized the field of computer vision and have been successfully applied to various tasks, including:\n",
        "\n",
        "1. **Image Classification:**\n",
        "   - CNNs can classify images into different categories or classes.\n",
        "   - They learn to recognize patterns and features in the images, enabling accurate classification even in the presence of variations and distortions.\n",
        "\n",
        "2. **Object Detection:**\n",
        "   - CNNs can detect and localize objects within an image.\n",
        "   - They can identify the presence and location of multiple objects, enabling applications like autonomous driving, surveillance, and robotics.\n",
        "\n",
        "3. **Image Segmentation:**\n",
        "   - CNNs can segment images into different regions or objects.\n",
        "   - They assign a class label or category to each pixel in the image, allowing for precise understanding and analysis of the image's content.\n",
        "\n",
        "CNNs have also been extended and modified to tackle more complex tasks, such as image generation, video analysis, and even natural language processing.\n",
        "\n",
        "By leveraging the hierarchical structure of CNNs, along with their ability to automatically learn and extract features from visual data, we can achieve state-of-the-art performance in various computer vision tasks.\n",
        "\n",
        "Experimenting with different CNN architectures, optimizing hyperparameters, and using transfer learning techniques are common practices to improve the performance and efficiency of CNN models in computer vision applications.\n",
        "\n",
        "By understanding the architecture and applications of CNNs, you are now equipped to design and train powerful deep learning models for computer vision tasks.\n",
        "\n",
        "Happy exploring and creating innovative computer vision applications with CNNs!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cdd317a3",
      "metadata": {},
      "source": [
        "## Pretrained Models and Transfer Learning\n",
        "\n",
        "Pretrained models and transfer learning are techniques used in deep learning to leverage existing models trained on large datasets for new tasks or domains. These techniques can save computation time and improve model performance, especially when working with limited labeled data.\n",
        "\n",
        "### Pretrained Models\n",
        "\n",
        "Pretrained models are deep learning models that have been trained on large datasets, such as ImageNet, which contains millions of labeled images. These models have learned to recognize a wide range of visual patterns and can be used as a starting point for similar tasks.\n",
        "\n",
        "By utilizing pretrained models, we can benefit from the learned features and weights, which capture general patterns and structures in the data. This can be particularly useful when working with limited training data, as the pretrained model has already learned meaningful representations from a large amount of labeled data.\n",
        "\n",
        "Popular pretrained models include VGG, ResNet, Inception, and MobileNet, among others. These models are often pretrained on large-scale image classification tasks and can be easily accessed and utilized through deep learning frameworks like TensorFlow or PyTorch.\n",
        "\n",
        "### Transfer Learning\n",
        "\n",
        "Transfer learning is the process of applying knowledge gained from a pretrained model to a new, related task. Instead of starting the training of a model from scratch, we can initialize the model with pretrained weights and adapt it to the new task using a smaller, task-specific dataset.\n",
        "\n",
        "Transfer learning offers several benefits:\n",
        "\n",
        "1. **Improved Performance:** Pretrained models have learned general features from large datasets, which can be particularly beneficial when working with limited labeled data. By leveraging these pretrained features, the model can quickly learn relevant patterns specific to the new task and achieve better performance compared to training from scratch.\n",
        "\n",
        "2. **Reduced Training Time:** By starting with pretrained weights, we save time and computational resources that would have been required to train the model from scratch. The pretrained model has already learned low-level and intermediate-level features that are useful for many different tasks. Thus, we can focus on fine-tuning the model's high-level features to adapt to the new task, which requires less training time.\n",
        "\n",
        "3. **Generalization:** Pretrained models have learned rich representations from diverse datasets. By transferring this knowledge, the model can generalize well to new, unseen data, even in different domains or tasks. The pretrained model has already captured high-level features that are relevant to various tasks, making it more adaptable and capable of handling different scenarios.\n",
        "\n",
        "To apply transfer learning, we typically freeze the early layers of the pretrained model, which capture general features, and only fine-tune the later layers to adapt to the specific task or dataset. By doing so, we ensure that the pretrained features are preserved while allowing the model to learn task-specific representations.\n",
        "\n",
        "Using transfer learning involves loading a pretrained model, modifying its architecture if necessary, and training it on the new task-specific dataset. Deep learning frameworks provide APIs to easily access and utilize pretrained models.\n",
        "\n",
        "For example, in Python using TensorFlow, we can load a pretrained VGG16 model as follows:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Load the pretrained VGG16 model\n",
        "model = VGG16(weights='imagenet', include_top=True)\n",
        "\n",
        "# Use the model for inference or fine-tuning\n",
        "# ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f02d5f00",
      "metadata": {},
      "source": [
        "## Advanced Techniques: Regularization and Optimization Techniques\n",
        "\n",
        "In deep learning, advanced techniques such as regularization and optimization play a crucial role in improving model performance, preventing overfitting, and achieving better generalization. Let's explore these techniques in detail:\n",
        "\n",
        "### Regularization Techniques\n",
        "\n",
        "Regularization techniques are used to prevent overfitting, which occurs when a model performs well on the training data but fails to generalize to new, unseen data. Overfitting can happen when a model becomes too complex and starts memorizing noise or irrelevant patterns in the training data.\n",
        "\n",
        "Two commonly used regularization techniques are:\n",
        "\n",
        "1. **L1 and L2 Regularization:**\n",
        "   - L1 and L2 regularization are methods to add a penalty term to the loss function during training.\n",
        "   - L1 regularization adds the absolute values of the weights to the loss function, promoting sparsity and encouraging some weights to become exactly zero.\n",
        "   - L2 regularization adds the squared values of the weights to the loss function, pushing the weights towards smaller values.\n",
        "   - By adding these penalty terms, the model is discouraged from relying too heavily on any particular feature and is encouraged to learn more generalizable patterns.\n",
        "\n",
        "2. **Dropout:**\n",
        "   - Dropout is a technique that randomly sets a fraction of the input units of a layer to zero during training.\n",
        "   - This helps in preventing over-reliance on specific neurons and encourages the model to learn more robust representations.\n",
        "   - Dropout introduces noise in the learning process, making the model more resilient and reducing overfitting.\n",
        "\n",
        "### Optimization Techniques\n",
        "\n",
        "Optimization techniques are used to improve the training process and find the optimal set of weights that minimize the loss function. Traditional optimization algorithms may struggle with deep neural networks due to the high dimensionality and non-convex nature of the loss landscape. Advanced optimization techniques overcome these challenges and speed up the training process.\n",
        "\n",
        "Two commonly used optimization techniques are:\n",
        "\n",
        "1. **Stochastic Gradient Descent (SGD):**\n",
        "   - SGD is a popular optimization algorithm used to update the network weights based on the gradients of the loss function.\n",
        "   - It performs updates on small batches of training data, making it computationally efficient and well-suited for large datasets.\n",
        "   - However, SGD may converge slowly and struggle with saddle points or local minima.\n",
        "\n",
        "2. **Adam Optimization:**\n",
        "   - Adam optimization is an extension of SGD that combines the advantages of both adaptive learning rates and momentum methods.\n",
        "   - It adapts the learning rate for each parameter based on the estimates of first and second moments of the gradients.\n",
        "   - Adam optimization is computationally efficient, requires less manual tuning of hyperparameters, and converges faster than traditional optimization algorithms.\n",
        "\n",
        "By applying regularization techniques and using advanced optimization algorithms, we can improve the generalization and training efficiency of deep learning models.\n",
        "\n",
        "## Data Augmentation\n",
        "\n",
        "Data augmentation is a technique used to artificially increase the size and diversity of the training data by applying various transformations or modifications to the existing data. This can help in reducing overfitting, improving model performance, and making the model more robust to variations in the input data.\n",
        "\n",
        "Common data augmentation techniques include:\n",
        "\n",
        "1. **Image Augmentation:**\n",
        "   - For image data, augmentation techniques such as rotation, translation, scaling, flipping, and cropping can be applied.\n",
        "   - These transformations create new variations of the images, allowing the model to learn from a more diverse set of examples.\n",
        "\n",
        "2. **Text Augmentation:**\n",
        "   - For text data, augmentation techniques such as word replacement, synonym replacement, and random insertion or deletion of words can be used.\n",
        "   - These techniques introduce variations in the text data, making the model more robust to different wordings or expressions.\n",
        "\n",
        "Data augmentation can be easily implemented using deep learning frameworks or libraries. By applying data augmentation, we can increase the effective size of the training data, improve model performance, and reduce the risk of overfitting.\n",
        "\n",
        "Experimenting with different augmentation techniques, adjusting the augmentation parameters, and evaluating the impact on model performance are important steps in effectively applying data augmentation.\n",
        "\n",
        "By incorporating regularization techniques, advanced optimization algorithms, and data augmentation into our deep learning workflow, we can enhance model performance, improve generalization, and build more robust and reliable deep learning models.\n",
        "\n",
        "Happy experimenting and applying these advanced techniques to your deep learning projects!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
