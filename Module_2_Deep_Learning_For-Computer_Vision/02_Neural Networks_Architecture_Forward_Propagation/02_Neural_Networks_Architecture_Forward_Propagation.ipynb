{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Architecture and Forward Propagation\n",
    "\n",
    "Neural networks are the foundation of deep learning. In this topic, we will explore the architecture of neural networks and understand how data propagates forward through them.\n",
    "\n",
    "## Neural Network Architecture\n",
    "\n",
    "Neural networks consist of interconnected nodes called neurons, organized in layers. The three main types of layers are input, hidden, and output layers.\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - The input layer receives the raw input data, which could be images, text, or any other form of data.\n",
    "   - Each neuron in the input layer represents a feature or attribute of the input.\n",
    "\n",
    "2. **Hidden Layers:**\n",
    "   - Hidden layers are layers between the input and output layers.\n",
    "   - They perform complex computations and extract relevant features from the input data.\n",
    "   - Deep neural networks have multiple hidden layers, allowing for more intricate learning.\n",
    "\n",
    "3. **Output Layer:**\n",
    "   - The output layer produces the final predictions or outputs of the model.\n",
    "   - The number of neurons in the output layer depends on the problem type.\n",
    "   - For example, in a binary classification task, there will be one neuron representing each class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"../images/FNNS.svg\" alt=\"Different architectures of Feedforward neural networks\" width=\"700\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram above illustrates the different architectures of Feedforward Neural Networks (FNN) within the neural network framework. These architectures include:\n",
    "\n",
    "- **Perceptron:** The Perceptron is the simplest form of a neural network, consisting of a single layer of neurons. It is typically used for binary classification tasks and linearly separable problems.\n",
    "\n",
    "- **Adaline (Adaptive Linear Neuron):** Adaline is an extension of the Perceptron that uses a linear activation function. It is suitable for regression tasks and problems that require continuous outputs.\n",
    "\n",
    "- **Multilayer Perceptron (MLP):** MLP is a type of FNN with one or more hidden layers between the input and output layers. It is capable of learning complex patterns and is widely used for various tasks such as image classification and natural language processing.\n",
    "\n",
    "- **Radial Basis Function Networks (RBFN):** RBFN uses radial basis functions as activation functions in the hidden layer. It is particularly effective for problems involving clustering, interpolation, and function approximation.\n",
    "\n",
    "- **Probabilistic Neural Networks (PNN):** PNN utilizes probabilistic approaches to model data and perform classification. It is useful for tasks involving pattern recognition and classification.\n",
    "\n",
    "- **Extreme Learning Machines (ELM):** ELM is characterized by a single hidden layer with randomly assigned connections and an output layer trained using least squares. It is known for its fast learning speed and has applications in various domains.\n",
    "\n",
    "Understanding the architecture and characteristics of these different FNN architectures can help in designing and developing effective deep learning models for a wide range of applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Forward Propagation\n",
    "\n",
    "Forward propagation, also known as feedforward, is a fundamental process in neural networks where data flows through the network from the input layer to the output layer. It is called \"forward\" because the information moves in a single direction without any feedback loops.\n",
    "\n",
    "In a neural network, information is represented and processed by interconnected nodes called neurons. These neurons are organized into layers: an input layer, one or more hidden layers, and an output layer. The input layer receives the initial input data, such as images, text, or numerical features. The hidden layers perform computations and extract relevant features from the input, while the output layer produces the final predictions or outputs.\n",
    "\n",
    "The key idea behind forward propagation is that each neuron in a layer receives inputs from the previous layer, performs a weighted sum of these inputs, applies an activation function, and passes the result to the next layer. This process is repeated layer by layer until the output layer is reached and the final predictions are obtained.\n",
    "\n",
    "During forward propagation, each neuron's weighted sum is calculated by multiplying the inputs by their corresponding weights, summing them up, and adding a bias term. The weighted sum represents a linear transformation of the input data. Then, an activation function is applied to introduce non-linearity and determine the output of the neuron. The activation function can map the weighted sum to a desired range or introduce complex non-linearities necessary for learning and modeling complex relationships in the data.\n",
    "\n",
    "By applying these steps in sequence, the neural network transforms the input data through various transformations and computations, ultimately producing predictions or outputs based on the learned weights and biases.\n",
    "\n",
    "The process by which data flows through the neural network from the input layer to the output layer. It involves several key steps:\n",
    "\n",
    "1. **Weighted Sum:**\n",
    "   - Each neuron in a layer receives inputs from the previous layer, multiplies them by corresponding weights, and sums them up.\n",
    "   - This step represents the linear transformation of the data.\n",
    "   - For example, consider a neuron in the hidden layer that receives inputs x1, x2, and x3 from the previous layer. The weighted sum can be calculated as follows: `weighted_sum = (w1 * x1) + (w2 * x2) + (w3 * x3) + bias`, where w1, w2, w3 are the weights and bias represents an additional learnable parameter.\n",
    "\n",
    "2. **Activation Function:**\n",
    "   - After the weighted sum, an activation function is applied to introduce non-linearity and determine the output of each neuron.\n",
    "   - Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
    "   - For example, the ReLU activation function returns the input if it is positive and 0 otherwise. It can be defined as follows: `output = max(0, weighted_sum)`.\n",
    "\n",
    "3. **Output Calculation:**\n",
    "   - The outputs from the activation functions in the previous layer serve as inputs to the next layer.\n",
    "   - This process continues until the output layer is reached, and the final predictions are obtained.\n",
    "   - For example, in a binary classification task, the output layer might use a sigmoid activation function to produce a probability value between 0 and 1, representing the likelihood of belonging to a particular class.\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Let's consider a simple example of a feedforward neural network for image classification:\n",
    "\n",
    "- Input Layer: The input layer consists of neurons that represent the pixels of an image. Each pixel value serves as a feature for the network.\n",
    "- Hidden Layers: The hidden layers perform computations and extract relevant features from the image. Each neuron in the hidden layers receives inputs from the previous layer and applies a weighted sum and activation function.\n",
    "- Output Layer: The output layer produces the final predictions, representing the probabilities of the input image belonging to different classes (e.g., cat, dog).\n",
    "\n",
    "During forward propagation, the input image is passed through the network. Each neuron in the hidden layers receives inputs from the previous layer, applies a weighted sum, and passes the result through an activation function. The output layer then produces the final predictions.\n",
    "\n",
    "This process of forward propagation allows the neural network to transform the input image and make predictions based on the learned weights and biases.\n",
    "\n",
    "By experimenting with different architectures and adjusting the weights and biases during the training process, neural networks can learn to recognize patterns and make accurate predictions on various tasks, including computer vision, natural language processing, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomendaciones de películas: 3\n",
      "Salida de la red neuronal: [0.10538447 0.0342273  0.48605407]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función de activación\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Parámetros de la red neuronal\n",
    "input_size = 5  # Cantidad de características de entrada\n",
    "hidden_size = 3  # Cantidad de neuronas en la capa oculta\n",
    "output_size = 5  # Cantidad de categorías de películas posibles\n",
    "\n",
    "# Valores de entrada (preferencias del usuario)\n",
    "input_data = np.array([0.8, 0.2, 0.5, 0.7, 0.9])\n",
    "\n",
    "# Pesos y sesgos\n",
    "w1 = np.random.randn(hidden_size, input_size)  # Pesos de la capa de entrada a la capa oculta\n",
    "b1 = np.random.randn(hidden_size)  # Sesgos de la capa oculta\n",
    "w2 = np.random.randn(output_size, hidden_size)  # Pesos de la capa oculta a la capa de salida\n",
    "b2 = np.random.randn(output_size)  # Sesgos de la capa de salida\n",
    "\n",
    "# Propagación hacia adelante\n",
    "h = np.dot(w1, input_data) + b1  # Cálculo del valor de entrada de la capa oculta\n",
    "a = sigmoid(h)  # Aplicación de la función de activación (en este caso, sigmoid) a la capa oculta\n",
    "output = np.dot(w2, a) + b2  # Cálculo del valor de entrada de la capa de salida\n",
    "\n",
    "# Salida (recomendaciones de películas)\n",
    "recommendations = np.argmax(output)  # Índice de la categoría con mayor probabilidad\n",
    "print(\"Recomendaciones de películas:\", recommendations)\n",
    "print(\"Salida de la red neuronal:\", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
